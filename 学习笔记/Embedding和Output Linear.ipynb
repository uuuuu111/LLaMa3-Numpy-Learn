{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbf61c9-25be-420e-ac45-8eade0aacc14",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "[没有思考过 Embedding，不足以谈 AI](https://zhuanlan.zhihu.com/p/643560252)\n",
    "\n",
    "看完这篇博客应该就对embedding是什么很理解了，而项目中代码的实现也就很清晰了，对于forward就是一个查表的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7e4b7-8ba5-4f75-a399-92ef998992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(np.random.randn(in_size, out_size), name='W')\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.W[x]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be856385-396a-431a-b99c-b246a556da41",
   "metadata": {},
   "source": [
    "# Output Linear\n",
    "也就是最后的输出.输出的是一个长度为词表大小向量\n",
    "## Output Linear 和 Embedding的权值共享\n",
    "预训练刚兴起时,在语言模型的**输出端Output Linear层重用Embedding层的权重是很常见的操作,比如BERT,第一版的T5,早期的GPT,都使用了这个操作,这是因为当模型主干部分不大且词表很大时,Embedding层的参数量很可观,如果输出端再新增一个独立的同样大小的权重矩阵的话,会导致显存消耗的激增.\n",
    "权值共享最直接的后果可能是预训练的初始损失非常大.因此现在随着模型参数规模的增大,Embedding层的占比相对变小了,这个方法也变得不常用."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
